name: Validate Chamber

on:
  workflow_call:
    inputs:
      aws_ips:
        required: true
        type: string
      os_ips:
        required: true
        type: string
      test_type:
        required: true
        type: string
  workflow_dispatch:
    inputs:
      aws_ips:
        required: true
        type: string
      os_ips:
        required: true
        type: string
      test_type:
        required: true
        type: string

jobs:
  validate:
    name: Run Chamber Validation
    runs-on: self-hosted  # Admin node (TB00 or UT03)

    steps:
      - name: Checkout test repo (cloud3.0-test-cases)
        uses: actions/checkout@v3
        with:
          repository: your-org/cloud3.0-test-cases
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          path: chamber-tests

      - name: Set up Ruby and dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ruby-full jq unzip rsync
          gem install bundler

      - name: Verify rvm.tar availability
        run: |
          if [ ! -f /tmp/rvm.tar ]; then
            cp chamber-tests/rvm.tar /tmp/rvm.tar
          fi

      - name: Update config.json with IPs and test type
        run: |
          CONFIG_PATH=chamber-tests/config/config.json
          jq \
            --arg aws_ip "${{ inputs.aws_ips }}" \
            --arg os_ip "${{ inputs.os_ips }}" \
            --arg test_type "${{ inputs.test_type }}" \
            '.chambers.test.nodes.aws = ($aws_ip | split(",")) | \
             .chambers.test.nodes.openstack = ($os_ip | split(",")) | \
             .chambers.test.test_type = $test_type' \
            "$CONFIG_PATH" > temp.json && mv temp.json "$CONFIG_PATH"

      - name: Run Cucumber Tests
        run: |
          cd chamber-tests
          chmod +x ./cloud3testautomationlauncher.sh
          ./cloud3testautomationlauncher.sh

      - name: Upload summary HTML report
        uses: actions/upload-artifact@v3
        with:
          name: cucumber-html-report
          path: chamber-tests/reports/summary.html

      - name: Upload full test reports
        uses: actions/upload-artifact@v3
        with:
          name: full-test-report
          path: chamber-tests/reports/

      - name: Send email summary
        if: always()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.example.com
          server_port: 465
          username: ${{ secrets.SMTP_USER }}
          password: ${{ secrets.SMTP_PASS }}
          subject: Chamber Test Report - ${{ inputs.test_type }}
          body: "Attached is the chamber validation summary report."
          to: vpasala@cadence.com
          attachments: chamber-tests/reports/summary.html















‚úÖ Deployment Functional Requirements (DFRs)
Use Case: Add new bare metal worker nodes (wrkNN) into an existing chamber (e.g., oa98) that already contains ls01 and haproxy.

1. üìÇ Chamber Assumptions
The target chamber (e.g., oa98) exists under:

swift
Copy
Edit
CustomerVPC/Terraform/config/envs/prod/prod1/oa98
It has an existing Terraform state (terraform.tfstate) and the following nodes:

ls01

haproxy

2. üõ†Ô∏è Terraform Configuration File (oa98.tfvars.json)
You must update the existing tfvars/config JSON:

‚úÖ a. Append to compute.compute_details.node_details:
For each new worker node:

json
Copy
Edit
"wrk04": {
  "name": "wrk04",
  "instance_type": "baremetal",
  "image": "<VALID_RHEL8.6_IMAGE_UUID>",
  "volume_size": 100,
  "additional_volumes": null,
  "eni_name": "wrk04-eni"
}
‚úÖ b. Append to networking.customer_eni_mapping:
For the ENI mapping of each worker:

json
Copy
Edit
"wrk04-eni": {
  "name": "wrk04-eni",
  "subnet": "ComputeSubnet2a",
  "security_groups": [
    "Chm-AccessFromUtlSvr",
    "PrivateSG",
    "CLA-SG",
    "Platform-SG"
  ],
  "ip": {
    "private_ip": "$${cc_chamber_internal_cidr}",
    "public_ip": "$${cc_chamber_cidr}",
    "hostnum": 104
  }
}
Choose unique hostnum for each node (e.g., 101, 102, 103, 104‚Ä¶)

ENI names must match eni_name from compute

3. üîÅ Dynamic Automation (Optional)
Use the Python script we built to dynamically:

Detect existing wrkNN nodes

Add new workers up to desired count

Auto-generate corresponding ENI blocks

4. üìú Terraform Commands
Navigate to the chamber path:
bash
Copy
Edit
cd CustomerVPC/Terraform/config/envs/prod/prod1/oa98
Initialize:
bash
Copy
Edit
terraform init
Plan and review changes:
bash
Copy
Edit
terraform plan -var-file="oa98.tfvars.json"
Apply changes:
bash
Copy
Edit
terraform apply -var-file="oa98.tfvars.json"
Or using Terragrunt:
bash
Copy
Edit
terragrunt apply
5. üß™ Validation After Apply
Confirm instance creation in OpenStack Horizon dashboard.

Ensure IPs and ENIs are correctly assigned.

Verify instance is reachable (ping or SSH).

Validate log forwarding if applicable (rsyslog to ls01).

6. üõ°Ô∏è Why These Values Are Needed
Field	Reason
name	Unique identifier for the node
instance_type	baremetal is mandatory for physical nodes
image	RHEL 8.6 or approved base image ID
eni_name	Required to map networking details
hostnum	Used for deterministic IP assignment via CIDR math
security_groups	Controls inbound/outbound traffic
subnet	Aligns with VPC subnet used for compute

7. üßæ Example Final Snippet (merged)
json
Copy
Edit
"wrk04": {
  "name": "wrk04",
  "instance_type": "baremetal",
  "image": "a3b8b3b7-0fe0-402c-9e35-8999dc07e564",
  "volume_size": 100,
  "additional_volumes": null,
  "eni_name": "wrk04-eni"
}
json
Copy
Edit
"wrk04-eni": {
  "name": "wrk04-eni",
  "subnet": "ComputeSubnet2a",
  "security_groups": [
    "Chm-AccessFromUtlSvr",
    "PrivateSG",
    "CLA-SG",
    "Platform-SG"
  ],
  "ip": {
    "private_ip": "$${cc_chamber_internal_cidr}",
    "public_ip": "$${cc_chamber_cidr}",
    "hostnum": 104
  }
}
Would you like me to generate the full updated .tfvars.json for wrk04‚Äìwrk06 as an example based on your chamber state?











Talk to Mia








Tools


